# 激活函数学习笔记

## 阶跃函数
## Sigmoid函数
## ReLU函数
## Softmax函数
## 等其他常见的激活函数

## 使用情况
1. 隐藏层
    - 首选ReLU，效果不好可以尝试Leakey ReLU
    - Sigmoid 在隐藏层容易导致梯度消失，应尽量避免
    - Tanh输出值为0，对中心化数据更加友好，但仍可能引发梯度消失，仅使用于浅层网络
2. 输出层
    - 二分类选择 sigmoid
    - 多分类选择 softmax
    - 回归问题默认选择 Identity